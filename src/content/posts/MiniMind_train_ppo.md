---
title: MiniMind PPO è®­ç»ƒæºç æ·±åº¦è§£æ
published: 2026-01-29
description: 'æ·±å…¥å‰–æ MiniMind é¡¹ç›®çš„ PPO (Proximal Policy Optimization) ç®—æ³•å®ç°ã€‚'
image: ''
tags: [LLM, Reinforcement Learning]
category: 'Reinforcement Learning'
draft: false
lang: 'zh'
priority: 5
---
![PPOç®—æ³•å®Œæ•´æµç¨‹å›¾](./images/PPO.png)
*å›¾ 1ï¼šPPO ç®—æ³•æ ¸å¿ƒæµç¨‹æ¶æ„å›¾ï¼ˆåŒ…å«æ•°æ®æ”¶é›†ã€GAE è®¡ç®—åŠ Actor-Critic åŒæµæ›´æ–°ï¼‰*

## PPO ç®—æ³•ä¸ MiniMind å¯¹é½å®æˆ˜å¯¼è¯»

åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒæµæ°´çº¿ä¸­ï¼Œå¦‚æœè¯´é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰èµ‹äºˆäº†æ¨¡å‹å¹¿åšçš„çŸ¥è¯†ï¼ŒSFTï¼ˆSupervised Fine-Tuningï¼‰æ•™ä¼šäº†æ¨¡å‹éµå¾ªæŒ‡ä»¤ï¼Œé‚£ä¹ˆ **RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰** åˆ™æ˜¯è®©æ¨¡å‹çœŸæ­£â€œå¯¹é½â€äººç±»ä»·å€¼è§‚ã€å­¦ä¼šæƒè¡¡ä¼˜åŠ£çš„å…³é”®æ­¥éª¤ã€‚

è€Œåœ¨ RLHF çš„ä¼—å¤šç®—æ³•ä¸­ï¼Œ**PPO (Proximal Policy Optimization)** å‡­å€Ÿå…¶åœ¨ç¨³å®šæ€§ä¸æ ·æœ¬æ•ˆç‡ä¹‹é—´çš„å‡ºè‰²å¹³è¡¡ï¼Œæˆä¸ºäº†ç›®å‰äº‹å®ä¸Šçš„è¡Œä¸šæ ‡å‡†ã€‚

æœ¬æ–‡å°†æ·±å…¥å‰–æ MiniMind é¡¹ç›®ä¸­çš„ `train_ppo.py` æºç ã€‚ä¸åŒäºæ•™ç§‘ä¹¦å¼çš„ç†è®ºè®²è§£ï¼Œæˆ‘ä»¬å°†ç›´æ¥ä»ä»£ç å®ç°çš„è§’åº¦ï¼Œè§£æ„ä¸€ä¸ªæ”¯æŒ **æ¨ç†èƒ½åŠ›å¢å¼ºï¼ˆReasoning-awareï¼‰** çš„ PPO è®­ç»ƒç³»ç»Ÿã€‚

é€šè¿‡é˜…è¯»æœ¬æ–‡ï¼Œä½ å°†ç†è§£ MiniMind æ˜¯å¦‚ä½•é€šè¿‡ä»¥ä¸‹æ ¸å¿ƒæ¨¡å—æ„å»ºå…¶å¼ºåŒ–å­¦ä¹ é—­ç¯çš„ï¼š

1. **å®Œæ•´çš„ Actor-Critic æ¶æ„**ï¼š
ä»£ç ä¸­ä¸ä»…å®ä¾‹åŒ–äº†ç”¨äºç”Ÿæˆçš„ **Actor Model**ï¼ˆç­–ç•¥ç½‘ç»œï¼‰ï¼Œè¿˜æ„å»ºäº†ä¸€ä¸ªåŸºäº `MiniMindLM` ä½†ä¿®æ”¹äº†è¾“å‡ºå±‚ï¼ˆ`value_head`ï¼‰çš„ **Critic Model**ï¼ˆä»·å€¼ç½‘ç»œï¼‰ã€‚ä¸ºäº†ä¿è¯è®­ç»ƒçš„æ•°å­¦ä¸¥è°¨æ€§ï¼Œç³»ç»Ÿè¿˜ç»´æŠ¤äº† **Old Actor**ï¼ˆç”¨äºè®¡ç®—æ¦‚ç‡æ¯”ç‡ ï¼‰å’Œ **Reference Model**ï¼ˆç”¨äºè®¡ç®— KL æ•£åº¦æƒ©ç½šï¼‰ï¼Œæ„æˆäº†ç»å…¸çš„â€œå››æ¨¡å‹â€äº¤äº’ç»“æ„ã€‚
2. **æ··åˆå¥–åŠ±å·¥ç¨‹ (Hybrid Reward Engineering)**ï¼š
è¿™æ˜¯æœ¬å®ç°çš„äº®ç‚¹ä¹‹ä¸€ã€‚åœ¨ `calculate_rewards` å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬ä¸ä»…å¼•å…¥äº†å¤–éƒ¨çš„ **Reward Model** å¯¹å›å¤è´¨é‡æ‰“åˆ†ï¼Œè¿˜é’ˆå¯¹æ¨ç†æ¨¡å‹ï¼ˆReasoning Modelï¼‰è®¾è®¡äº† **åŸºäºè§„åˆ™çš„æ ¼å¼å¥–åŠ±**ã€‚é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ï¼ˆRegexï¼‰ä¸¥æ ¼çº¦æŸ `<think>` å’Œ `<answer>` æ ‡ç­¾çš„ç»“æ„ï¼Œå¹¶å¼•å…¥ç¨€ç–æ ‡è®°å¥–åŠ±ï¼Œå¼ºåˆ¶æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­å­¦ä¼šâ€œå…ˆæ€è€ƒï¼Œåå›ç­”â€çš„æ€ç»´é“¾æ¨¡å¼ã€‚
3. **PPO æ ¸å¿ƒç›®æ ‡å‡½æ•°**ï¼š
æˆ‘ä»¬å°†é€è¡Œæ‹†è§£ `ppo_train_epoch` ä¸­çš„æŸå¤±è®¡ç®—é€»è¾‘ï¼š
* **Policy Loss**ï¼šåˆ©ç”¨è¿™ä¸€æ—¶åˆ»çš„ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantageï¼‰å’Œæ¦‚ç‡æ¯”ç‡ï¼ˆRatioï¼‰ï¼Œé…åˆ PPO æ ‡å¿—æ€§çš„ **Clipping æœºåˆ¶**ï¼ˆ`clip_epsilon`ï¼‰ï¼Œé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œé˜²æ­¢æ¨¡å‹â€œå­¦å´©â€ã€‚
* **Value Loss**ï¼šé€šè¿‡ MSE æŸå¤±è®© Critic ç½‘ç»œæ›´å‡†ç¡®åœ°é¢„ä¼°å½“å‰çŠ¶æ€çš„ä»·å€¼ã€‚
* **KL Divergence Penalty**ï¼šä¸ºäº†é˜²æ­¢æ¨¡å‹åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­è¿‡åº¦åç¦» SFT åçš„åŸºåº§æ¨¡å‹ï¼ˆReward Hackingï¼‰ï¼Œæˆ‘ä»¬åœ¨æ€»æŸå¤±ä¸­åŠ å…¥äº†åŠ¨æ€çš„ KL æ•£åº¦æƒ©ç½šé¡¹ã€‚


4. **å·¥ç¨‹åŒ–å®ç°ç»†èŠ‚**ï¼š
ä» `DistributedDataParallel` (DDP) çš„åˆ†å¸ƒå¼å°è£…ï¼Œåˆ°å¤„ç†å˜é•¿åºåˆ—çš„ `Mask` æŠ€å·§ï¼Œå†åˆ°å·¦ä¾§å¡«å……ï¼ˆLeft Paddingï¼‰å¯¹ç”Ÿæˆè¿‡ç¨‹çš„å½±å“ï¼Œæœ¬æ–‡å°†å±•ç¤ºå¦‚ä½•åœ¨ä¸€ä¸ªçœŸå®çš„ PyTorch ç¯å¢ƒä¸­é«˜æ•ˆã€ç¨³å®šåœ°è¿è¡Œ PPOã€‚

è®©æˆ‘ä»¬è·Ÿéšä»£ç ï¼Œçœ‹ä¸€çœ‹è¿™å¥—å¤æ‚çš„â€œæ•°å­—é½¿è½®â€æ˜¯å¦‚ä½•ç²¾å¯†å’¬åˆçš„ã€‚

---

## å…¨å±€å¼•ç”¨ä¸ç¯å¢ƒåˆå§‹åŒ– (Imports & Setup)

<details>
<summary><strong>ğŸ‘‰ ç‚¹å‡»å±•å¼€æŸ¥çœ‹å®Œæ•´å¼•ç”¨ä¸ç¯å¢ƒåˆå§‹åŒ–ä»£ç </strong></summary>

```python title="train_ppo.py"
import os
import sys

__package__ = "trainer" #å½“å‰ä»£ç å±äºtrainerè¿™ä¸ªåŒ…
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))) 
#é¡¹ç›®æ ¹ç›®å½•

import argparse
import re
import warnings
import torch
import torch.distributed as dist
import torch.nn.functional as F
from transformers import AutoTokenizer
from contextlib import nullcontext
from torch import optim, nn
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data import DataLoader, DistributedSampler
from torch.nn.utils import clip_grad_norm_
from torch.optim.lr_scheduler import CosineAnnealingLR
from transformers import AutoModel
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM
from dataset.lm_dataset import RLAIFDataset
from trainer.trainer_utils import Logger, is_main_process, lm_checkpoint, init_distributed_mode, setup_seed, SkipBatchSampler, init_model

warnings.filterwarnings('ignore')
```
</details>

## æ ¸å¿ƒæ¶æ„è®¾è®¡ (The Critic Model)

```python title="train_ppo.py"
# è‡ªå®šä¹‰çš„Criticæ¨¡å‹ï¼Œç»§æ‰¿è‡ªMiniMindForCausalLM
class CriticModel(MiniMindForCausalLM):
    def __init__(self, params):
        super().__init__(params)
        # å°†åŸæœ‰çš„lm_head(è¾“å‡ºvocab_sizeç»´åº¦)æ›¿æ¢ä¸ºvalue_head(è¾“å‡º1ç»´åº¦ï¼Œå³å¯¹å½“å‰çŠ¶æ€è¿›è¡Œä»·å€¼æ‰“åˆ†)
        self.value_head = nn.Linear(params.hidden_size, 1)

    def forward(self, input_ids=None, attention_mask=None, **kwargs):
        # ä½¿ç”¨åŸºç¡€æ¨¡å‹è·å–éšè—çŠ¶æ€:[Batch_size, Sequence_Length, Hidden_size]
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)
        hidden_states = self.model.norm(outputs[0])
        # ä½¿ç”¨value_headè·å–ä»·å€¼ä¼°è®¡
        # value_headä¹‹åï¼Œå¼ é‡çš„å½¢çŠ¶å˜ä¸º[B, L, 1], squeeze(-1)å»æ‰æœ€åä¸€ä¸ªç»´åº¦ï¼Œå³valueså˜ä¸º[B, L]
        values = self.value_head(hidden_states).squeeze(-1)
        return values

```


## æ··åˆå¥–åŠ±æœºåˆ¶ (Hybrid Reward Engineering)

ä¸ºäº†è®©æ¨¡å‹ä¹ å¾—â€œDeepSeek-R1â€å¼çš„æ€è€ƒæ¨¡å¼ï¼Œå•çº¯ä¾èµ–ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰æ˜¯ä¸å¤Ÿçš„ã€‚æˆ‘ä»¬éœ€è¦é€šè¿‡æ··åˆå¥–åŠ±æœºåˆ¶ï¼Œæ˜¾å¼åœ°å¼•å¯¼æ¨¡å‹ç”Ÿæˆç¬¦åˆ `<think>...</think><answer>...</answer>` ç»“æ„çš„å›å¤ã€‚

### 1. æ ¼å¼è§„èŒƒå¥–åŠ± (Format Compliance Reward)

é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå†…éƒ¨å‡½æ•° `reasoning_model_reward`ï¼Œå®ƒåˆ©ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼ˆRegexï¼‰æ¥å¼ºåˆ¶çº¦æŸæ¨¡å‹çš„è¾“å‡ºæ ¼å¼ã€‚å¦‚æœæ¨¡å‹èƒ½å®Œç¾ç”ŸæˆåŒ…å«æ€è€ƒå’Œå›ç­”æ ‡ç­¾çš„ç»“æ„ï¼Œç»™äºˆ `0.5` çš„ç¡¬æ€§å¥–åŠ±ã€‚è¿™æœ‰åŠ©äºæ¨¡å‹åœ¨åˆæœŸå¿«é€Ÿé€šè¿‡å¼ºåŒ–å­¦ä¹ â€œå­¦ä¼šâ€è¿™ç§ç‰¹å®šçš„è¾“å‡ºèŒƒå¼ã€‚

```python
    def reasoning_model_reward(rewards):
        # 1. æ ¼å¼å¥–åŠ±ï¼ˆä»…é’ˆå¯¹è®­ç»ƒæ¨ç†æ¨¡å‹æ—¶ä½¿ç”¨ï¼‰
        # ä¸¤ç§åŒ¹é…æ¨¡å¼, patternæ˜¯</think>ä¸<answer>ä¹‹é—´æ²¡æœ‰å¤šä½™ç©ºè¡Œï¼Œpattern2æ˜¯å…è®¸ä¸€ä¸ªå¤šä½™ç©ºè¡Œ
        pattern = r"^<think>\n.*?\n</think>\n<answer>\n.*?\n</answer>$"
        pattern2 = r"^<think>\n.*?\n</think>\n\n<answer>\n.*?\n</answer>$"
        #åˆ—è¡¨åªä¼šå‡ºç°ä¸¤ç§æƒ…å†µï¼Œre.Matchå¯¹è±¡æˆ–è€…None(åŒ¹é…å¤±è´¥)
        matches_pattern = [re.match(pattern, response, re.S) for response in responses]
        matches_pattern2 = [re.match(pattern2, response, re.S) for response in responses]

        format_rewards = []
        for match_pattern, match_pattern2 in zip(matches_pattern, matches_pattern2):
            if match_pattern:
                format_rewards.append(0.5)
            elif match_pattern2:
                format_rewards.append(0.5) # å…è®¸ä¸­é—´å¤šä¸€ä¸ªæ¢è¡Œ
            else:
                format_rewards.append(0.0)
        #è½¬åŒ–ä¸ºå¼ é‡
        rewards += torch.tensor(format_rewards, device=args.device)

```

### 2. ç¨€ç–æ ‡è®°å¥–åŠ± (Sparse Tag Reward)

ä¸ºäº†é˜²æ­¢è®­ç»ƒåˆæœŸçš„å¥–åŠ±è¿‡äºç¨€ç–ï¼ˆå³æ¨¡å‹å¾ˆéš¾ä¸€å¼€å§‹å°±å®Œç¾åŒ¹é…æ•´ä¸ªæ­£åˆ™ï¼‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»†ç²’åº¦çš„æ ‡è®°å¥–åŠ±ã€‚åªè¦æ¨¡å‹è¾“å‡ºäº†æ­£ç¡®çš„ `<think>` æˆ– `<answer>` æ ‡ç­¾ï¼Œæ¯ä¸ªæ ‡ç­¾å•ç‹¬ç»™äºˆ `0.25` çš„å¥–åŠ±ã€‚è¿™ç§â€œç§¯å°‘æˆå¤šâ€çš„ç­–ç•¥èƒ½æœ‰æ•ˆå¼•å¯¼æ¨¡å‹é€æ­¥é€¼è¿‘æœ€ç»ˆçš„æ­£ç¡®æ ¼å¼ã€‚

```python
        # 2. æ ‡è®°å¥–åŠ±ï¼ˆé˜²æ­¢ä¸¥æ ¼å¥–åŠ±ç¨€ç–ï¼Œä»…é’ˆå¯¹è®­ç»ƒæ¨ç†æ¨¡å‹æ—¶ä½¿ç”¨ï¼‰
        def mark_num(text):
            reward = 0
            # åªè¦å‡ºç°äº†å¯¹åº”çš„æ ‡ç­¾ï¼Œå°±ç»™äºˆéƒ¨åˆ†å¥–åŠ±
            if text.count("<think>") == 1:
                reward += 0.25
            if text.count("</think>") == 1:
                reward += 0.25
            if text.count("<answer>") == 1:
                reward += 0.25
            if text.count("</answer>") == 1:
                reward += 0.25
            return reward
        mark_rewards = [mark_num(response) for response in responses]
        rewards += torch.tensor(mark_rewards, device=args.device)
        return rewards

```

### 3. å†…å®¹è¯­ä¹‰å¥–åŠ±ä¸åŠ æƒ (Semantic Content Reward)

æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å¤–éƒ¨çš„ Reward Modelå¯¹å†…å®¹çš„å®è´¨è´¨é‡è¿›è¡Œæ‰“åˆ†ã€‚

å¯¹äºæ¨ç†æ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ç‰¹æ®Šçš„åŠ æƒç­–ç•¥ï¼š

1. è®¡ç®—**å…¨æ®µå›å¤**ï¼ˆPrompt + Think + Answerï¼‰çš„å¾—åˆ†ã€‚
2. æå– `<answer>` æ ‡ç­¾å†…çš„**çº¯å›ç­”å†…å®¹**ï¼Œå†æ¬¡è®¡ç®—å¾—åˆ†ã€‚
3. åŠ æƒå¾—åˆ°æœ€ç»ˆå¾—åˆ† 

è¿™ç§åŠ æƒæœºåˆ¶ï¼ˆ0.4/0.6ï¼‰ç¨å¾®åå‘äºæœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¹Ÿå…¼é¡¾äº†æ€è€ƒè¿‡ç¨‹çš„åˆç†æ€§ã€‚

```python
    with torch.no_grad():
        reward_model_scores = []
        for prompt, response in zip(prompts, responses):
            #Reward Modelè¯„ä»·çš„æ˜¯å¯¹è¯ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªå¥å­ã€‚ä¾‹å¦‚å¦‚æœç”¨æˆ·è¦æ±‚è¾“å‡ºä¸€å¥æœ‰è¯­ç—…çš„å¥å­ï¼Œå¦‚æœä»…ä»…æ˜¯å¯¹å¥å­è¯„ä»·ï¼Œé‚£ä¹ˆReward Modelä¼šç»™è¿™ä¸ªè¯­ç—…å¥å­ç»™å‡ºå¾ˆä½çš„åˆ†æ•°ï¼Œä½†æ˜¯è¿™æ»¡è¶³äº†ç”¨æˆ·çš„éœ€æ±‚ï¼Œæ‰€ä»¥åº”è¯¥ç»™ä¸€ä¸ªå¾ˆé«˜çš„åˆ†æ•°

            #å¯¹å›ç­”è¿›è¡Œè§£åŒ…ï¼Œè§£ææˆChat Formatæ ¼å¼
            pattern = r"<\|im_start\|>(system|user|assistant)\s+(.*?)<\|im_end\|>"
            matches = re.findall(pattern, prompt, re.DOTALL)
            messages = [{"role": role, "content": content.strip()} for role, content in matches]

            tmp_chat = messages + [{"role": "assistant", "content": response}]
            #Reward Modelè¿›è¡Œæ‰“åˆ†
            score = reward_model.get_score(reward_tokenizer, tmp_chat)
            #å¥–åŠ±æˆªæ–­ï¼Œå°†å¥–åŠ±åˆ†æ•°æ§åˆ¶åœ¨[-3.0, 3.0]è¿™ä¸ªåŒºé—´
            scale = 3.0
            score = max(min(score, scale), -scale)

            # å½“args.reasoning=1æ—¶ï¼Œé¢å¤–è®¡ç®—<answer>å†…å®¹çš„å¥–åŠ±
            if args.reasoning == 1:
                answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)
                if answer_match:
                    answer_content = answer_match.group(1).strip()
                    # å¯¹answerå†…å®¹å•ç‹¬è®¡ç®—reward
                    # ä¼ªé€ ä¸€ä¸ªå¯¹è¯ï¼Œä»…ä»…åŒ…å«ç­”æ¡ˆï¼Œå³å‡è£…æ¨¡å‹æ²¡æœ‰åºŸè¯ï¼Œç›´æ¥å›ç­”ç­”æ¡ˆ
                    tmp_chat = messages + [{"role": "assistant", "content": answer_content}]
                    # å¯¹è¿™ä¸ªä»…åŒ…æ‹¬ç­”æ¡ˆçš„å¯¹è¯è¿›è¡Œæ‰“åˆ†ï¼Œå¹¶è¿›è¡Œæˆªæ–­
                    answer_score = reward_model.get_score(reward_tokenizer, tmp_chat)
                    answer_score = max(min(answer_score, scale), -scale)
                    # å¯¹åˆ†æ•°è¿›è¡ŒåŠ æƒæ··åˆå¤„ç†
                    score = score * 0.4 + answer_score * 0.6
            reward_model_scores.append(score)

        reward_model_scores = torch.tensor(reward_model_scores, device=args.device)
        rewards += reward_model_scores

    return rewards


```

## PPO è®­ç»ƒå¾ªç¯æ ¸å¿ƒ (PPO Training Loop)

### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒè®¾ç½®ä¸æ•°æ®ç”Ÿæˆ (Rollout Phase)

è¿™ä¸€æ­¥æ˜¯ RL çš„â€œæ¢ç´¢â€é˜¶æ®µã€‚Actor æ¨¡å‹åŸºäºå½“å‰çš„ Prompt ç”Ÿæˆå›å¤ï¼ˆé‡‡æ ·ï¼‰ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸è®¡ç®—æ¢¯åº¦ï¼ˆ`no_grad`ï¼‰ï¼Œä¸»è¦ç›®çš„æ˜¯è·å–â€œç»éªŒæ•°æ®â€ã€‚

```python
def ppo_train_epoch(epoch, loader, iters, old_actor_model, ref_model, actor_scheduler, critic_scheduler, reward_model, reward_tokenizer, start_step=0, wandb=None):
    actor_model.train()
    critic_model.train()

    for step, batch in enumerate(loader, start=start_step + 1):
        prompts = batch["prompt"]  # list[str], length B
        # 1. ç¼–ç  Prompt
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ padding=True å’Œ truncation=True ç¡®ä¿ batch å†…ç»´åº¦ä¸€è‡´ (æˆªæ–­å’Œè¡¥é½æ“ä½œ)
        # enc æ˜¯ä¸€ä¸ª BatchEncoding å¯¹è±¡, å…¶å«æœ‰ä¸¤ä¸ªæœ€æ ¸å¿ƒçš„æˆå‘˜:
        # input_ids:[Batch_Size, Prompt_Length], å°†å•è¯/å­—ç¬¦æ˜ å°„ä¸ºè¯è¡¨ä¸­çš„ç´¢å¼•ID
        # attention_mask:[Batch_Size, Prompt_Length], 
        enc = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, 
                       max_length=args.max_seq_len).to(args.device)  # input_ids: [B, P], attention_mask: [B, P]
        
        # ä½¿ç”¨æ ‡é‡è®°å½• Prompt çš„å®é™…é•¿åº¦, å› ä¸ºç»è¿‡å·¦ä¾§å¡«å……, æ‰€æœ‰Prompt_Lengthéƒ½ä¸€è‡´
        prompt_length = enc.input_ids.shape[1]

        with torch.no_grad():
            # DDP æ¨¡å‹éœ€è¦ä½¿ç”¨ .module è®¿é—® generate æ–¹æ³•ï¼Œè¿™æ˜¯ PyTorch DDP çš„ç‰¹æ€§
            model_for_gen = actor_model.module if isinstance(actor_model, DistributedDataParallel) else actor_model
            
            # temperature=0.8 å¢åŠ äº†ä¸€å®šçš„éšæœºæ€§ï¼Œé˜²æ­¢ç­–ç•¥è¿‡æ—©æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ã€‚
            gen_out = model_for_gen.generate(
                input_ids=enc.input_ids, attention_mask=enc.attention_mask,
                max_new_tokens=args.max_gen_len, do_sample=True, temperature=0.8,
                pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)  # [B, P+R], å³åŸå§‹æ¨¡å‹é•¿åº¦+Response Length, å³æ¨¡å‹æ–°ç”Ÿæˆçš„å›ç­”, åŒ…å«äº†å®Œæ•´çš„å¯¹è¯å†…å®¹

```

### ç¬¬äºŒæ­¥ï¼šå¥–åŠ±è®¡ç®—ä¸ä¼˜åŠ¿ä¼°è®¡ (Reward & Advantage)

è·å¾—ç”Ÿæˆçš„æ–‡æœ¬åï¼Œæˆ‘ä»¬éœ€è¦è¯„ä»·å®ƒå¥½ä¸å¥½ï¼ˆè®¡ç®— Rewardï¼‰ï¼Œå¹¶ä½¿ç”¨ Critic æ¨¡å‹é¢„ä¼°å½“å‰çŠ¶æ€çš„ä»·å€¼ï¼ˆValueï¼‰ï¼Œè¿›è€Œè®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantageï¼‰ã€‚

```python

        # ä»ç”Ÿæˆçš„ token ä¸­åˆ‡ç‰‡æå–å‡º Response éƒ¨åˆ†è¿›è¡Œè§£ç 
        # è¿™é‡Œçš„len(prompts)å°±æ˜¯Batch_size
        responses_text = [tokenizer.decode(gen_out[i, prompt_length:], skip_special_tokens=True) for i in range(len(prompts))]
        
        # è°ƒç”¨ä¹‹å‰å®šä¹‰çš„ calculate_rewardsï¼ŒåŒ…å«æ ¼å¼å¥–åŠ±ã€æ ‡è®°å¥–åŠ±å’Œ Reward Model æ‰“åˆ†
        rewards = calculate_rewards(prompts, responses_text, reward_model, reward_tokenizer)  # [B]

        # æ„å»ºå…¨åºåˆ—çš„ maskï¼Œå› ä¸º Critic éœ€è¦çœ‹åˆ°å®Œæ•´çš„ input_ids
        full_mask = (gen_out != tokenizer.pad_token_id).long()  # [B, P+R]çš„çŸ©é˜µï¼Œä¸æ˜¯Paddingçš„åœ°æ–¹ä¸º1ï¼Œæ˜¯ä¸º0
        values_seq = critic_model(input_ids=gen_out, attention_mask=full_mask)  # [B, P+R]ï¼Œå¯¹äºæ¯ä¸€ä¸ªä½ç½®ï¼Œéƒ½ç»™å‡ºäº†ä¸€ä¸ªæ ‡é‡åˆ†æ•°
        
        # å› ä¸ºarangeæ˜¯é€’å¢åºåˆ—ï¼Œæ‰€ä»¥æœ€å¤§çš„å°±æ˜¯æœ€åä¸€ä¸ªæœ‰æ•ˆTokençš„ä½ç½®
        last_indices = (full_mask * torch.arange(full_mask.size(1), device=gen_out.device)).argmax(dim=1)
        
        # æå–æœ€åä¸€ä¸ª token å¯¹åº”çš„ value ä½œä¸ºå½“å‰ç”Ÿæˆçš„æ•´ä½“ä»·å€¼é¢„ä¼°
        # arangeç”Ÿæˆä¸€ä¸ª[0, 1, ..., B-1]çš„è¡Œåæ ‡, å†æ ¹æ®last_indicesçš„çºµåæ ‡, æå–å‡ºæœ€åä¸€ä¸ªæœ‰æ•ˆTokençš„ç´¢å¼•
        values = values_seq[torch.arange(values_seq.size(0), device=values_seq.device), last_indices]  # [B]
        
        # Advantage = å®é™…è·å¾—çš„å¥–åŠ± - Critic é¢„ä¼°çš„ä»·å€¼
        # .detach() å¾ˆå…³é”®ï¼šè®¡ç®— Advantage æ—¶ä¸åå‘ä¼ æ’­æ¢¯åº¦ç»™ Critic
        advantages = rewards - values.detach()  # [B]

```

### ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—å½“å‰ç­–ç•¥ä¸å‚è€ƒç­–ç•¥çš„æ¦‚ç‡ (Log Probabilities)

è¿™æ˜¯ PPO ä¸­æœ€â€œé‡â€è®¡ç®—çš„ä¸€æ­¥ã€‚æˆ‘ä»¬éœ€è¦åˆ†åˆ«è®¡ç®—ä¸‰ä¸ªæ¨¡å‹ï¼ˆå½“å‰ Actorã€æ—§ Actorã€å‚è€ƒ Refï¼‰å¯¹åŒä¸€æ¡ç”Ÿæˆåºåˆ—çš„å¯¹æ•°æ¦‚ç‡ã€‚

```python

        # å‰å‘ä¼ æ’­è·å– logits
        # æ‹¿å·²ç»ç”Ÿæˆçš„å›ç­”å’Œè¾“å…¥å‰å‘ä¼ æ’­ä¸€éï¼Œå°±å¾—åˆ°äº†è¯è¡¨ä¸­å„ä¸ªtokençš„"åˆ†æ•°"ï¼Œæˆ‘ä»¬åç»­è¦ç”¨åˆ°
        logits = actor_model(input_ids=gen_out, attention_mask=full_mask).logits  # [B, P+R, V]
        labels = gen_out[:, 1:].clone()  # [B, P+R-1] (Labels æ˜¯ input å‘åé”™ä¸€ä½)
        
        # è·å–ç”Ÿæˆ token å¯¹åº”çš„ log_softmax å€¼
        # [:, :-1]è¦å»æ‰æœ€åä¸€ä¸ªtoken, gatherå–å‡ºè¾“å‡ºtokençš„åˆ†æ•°ï¼Œå†é™ç»´
        logp_tokens = F.log_softmax(logits[:, :-1], dim=-1).gather(2, labels.unsqueeze(-1)).squeeze(-1)  # [B, P+R-1]
        
        # åˆ›å»º mask ä»¥å±è”½ Prompt éƒ¨åˆ†ï¼ˆæˆ‘ä»¬åªå…³å¿ƒ Response çš„æ¦‚ç‡ï¼‰å’Œ Padding éƒ¨åˆ†
        seq_len = gen_out.size(1) - 1
        # ä½¿ç”¨æ ‡é‡ prompt_length è¿›è¡Œæ¯”è¾ƒ
        resp_mask = torch.arange(seq_len, device=gen_out.device).unsqueeze(0) >= prompt_length - 1
        # æ‰¾åˆ°æ˜¯Paddingçš„éƒ¨åˆ†, å¦‚æœä¸ºTrueï¼Œå¿…å®šä¸æ˜¯Paddingä¸”æ˜¯Response
        final_mask = resp_mask & (~labels.eq(tokenizer.pad_token_id))  # [B, P+R-1]
        
        # æ±‚å’Œå¾—åˆ°æ•´å¥ Response çš„ log probability
        # åŠ æ˜¯å› ä¸ºlog(a*b)=log(a)+log(b)
        actor_logp = (logp_tokens * final_mask).sum(dim=1)  # [B]

        # 8. è®¡ç®— Old Actor å’Œ Ref Model çš„ Log Prob (ä¸è®¡ç®—æ¢¯åº¦)
        with torch.no_grad():
            # Old Actor: ç”¨äºè®¡ç®— PPO çš„æ¦‚ç‡æ¯”ç‡ (Ratio)
            # å’Œä¸Šè¿°åŒæ ·çš„åŸç†
            old_logits = old_actor_model(input_ids=gen_out, attention_mask=full_mask).logits  # [B, P+R, V]
            old_logp_tokens = F.log_softmax(old_logits[:, :-1], dim=-1).gather(2, labels.unsqueeze(-1)).squeeze(-1)  # [B, P+R-1]
            old_logp = (old_logp_tokens * final_mask).sum(dim=1)  # [B]
            
            # Reference Model: ç”¨äºè®¡ç®— KL æ•£åº¦æƒ©ç½šï¼Œé˜²æ­¢æ¨¡å‹è·‘å
            # å’Œä¸Šè¿°åŒæ ·çš„åŸç†
            ref_logits = ref_model(input_ids=gen_out, attention_mask=full_mask).logits  # [B, P+R, V]
            ref_logp_tokens = F.log_softmax(ref_logits[:, :-1], dim=-1).gather(2, labels.unsqueeze(-1)).squeeze(-1)  # [B, P+R-1]
            ref_logp = (ref_logp_tokens * final_mask).sum(dim=1)  # [B]

```

### ç¬¬å››æ­¥ï¼šæ„å»º PPO æŸå¤±å‡½æ•° (Loss Calculation)

è¿™é‡Œå®ç°äº† PPO è®ºæ–‡çš„æ ¸å¿ƒå…¬å¼ï¼š<div style="overflow-x: auto; padding: 10px;">
$$
\mathcal{L}_{total}(\theta, \phi) = \frac{1}{B} \sum_{i=1}^{B} \left( - \min \left( r_i(\theta) \hat{A}_i, \ \text{clip}(r_i(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_i \right) + \lambda_{vf} (V_\phi(x_i, y_i) - R_i)^2 + \lambda_{kl} \log \frac{\pi_\theta(y_i|x_i)}{\pi_{ref}(y_i|x_i)} \right)
$$
</div>

```python
        # E[log(p)-log(q)], mean()å³ä»£è¡¨å–å¹³å‡ï¼Œä¹Ÿå°±æ˜¯EæœŸæœ›
        # è¿™é‡Œæ˜¯æ•´å¥è¯çš„KLæ•£åº¦, è€Œä¸æ˜¯ä¸€ä¸ªTokençš„KLæ•£åº¦
        kl = (actor_logp - old_logp).mean()  
        kl_ref = (actor_logp - ref_logp).mean()  
        
        # ratio = exp(log(new) - log(old)) = new / old
        ratio = torch.exp(actor_logp - old_logp)  # [B]
        # æœªæˆªæ–­çš„æŸå¤±
        surr1 = ratio * advantages  # [B] 

        # æˆªæ–­ ratio åœ¨ [1-epsilon, 1+epsilon] ä¹‹é—´
        surr2 = torch.clamp(ratio, 1.0 - args.clip_epsilon, 1.0 + args.clip_epsilon) * advantages  # [B]
        
        # Policy Loss: å–æœ€å°å€¼çš„è´Ÿæ•°ï¼ˆå› ä¸ºæ˜¯æ¢¯åº¦ä¸‹é™ï¼Œè¦æœ€å¤§åŒ–ç›®æ ‡å‡½æ•°ï¼‰
        policy_loss = -torch.min(surr1, surr2).mean()  # scalar
        
        # Value Loss: MSE Lossï¼Œè®© Critic æ›´å‡†
        value_loss = F.mse_loss(values, rewards)  # scalar
        
        # 12. æ€» Loss
        # Loss = Policy Loss + c1 * Value Loss + c2 * KL Penalty
        loss = policy_loss + args.vf_coef * value_loss + args.kl_coef * kl_ref  # scalar
        loss.backward()

```

### ç¬¬äº”æ­¥ï¼šåå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–° (Optimization)

æ ‡å‡†çš„ PyTorch ä¼˜åŒ–æ­¥éª¤ï¼ŒåŒ…å«æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰å’Œæ¢¯åº¦ç´¯ç§¯ã€‚

```python
        # æ¢¯åº¦æ›´æ–° (æ”¯æŒæ¢¯åº¦ç´¯ç§¯)
        if (step + 1) % args.accumulation_steps == 0:
            clip_grad_norm_(actor_model.parameters(), args.grad_clip)
            clip_grad_norm_(critic_model.parameters(), args.grad_clip)
            
            actor_optimizer.step()
            critic_optimizer.step()
            actor_scheduler.step()
            critic_scheduler.step()
            
            actor_optimizer.zero_grad()
            critic_optimizer.zero_grad()
            torch.cuda.empty_cache() # æ¸…ç†æ˜¾å­˜ç¢ç‰‡

```

### ç¬¬å…­æ­¥ï¼šæ—¥å¿—è®°å½•ä¸æ¨¡å‹ç»´æŠ¤ (Logging & Maintenance)

è®­ç»ƒå¾ªç¯çš„æœ€åéƒ¨åˆ†ï¼Œè´Ÿè´£å‘ WandB å‘é€æ•°æ®ï¼Œå®šæœŸåŒæ­¥ Old Actorï¼Œå¹¶ä¿å­˜æ¨¡å‹æƒé‡ã€‚

```python
        # æ—¥å¿—è®°å½•
        if is_main_process():
            # è®¡ç®—å¹³å‡ç”Ÿæˆé•¿åº¦ï¼Œç”¨äºç›‘æ§æ¨¡å‹æ˜¯å¦å‡ºç°â€œæ²‰é»˜â€æˆ–â€œå•°å—¦â€å€¾å‘
            response_ids = gen_out[:, enc.input_ids.shape[1]:]
            is_eos = (response_ids == tokenizer.eos_token_id)
            eos_indices = torch.argmax(is_eos.int(), dim=1)
            # åˆ¤æ–­æ ·æœ¬æ˜¯ä¸æ˜¯çœŸçš„æœ‰<eos>
            has_eos = is_eos.any(dim=1)
            # è®¡ç®—æ ·æœ¬å®é™…ç”Ÿæˆçš„æœ‰æ•ˆé•¿åº¦
            lengths = torch.where(has_eos, eos_indices + 1, torch.tensor(response_ids.shape[1], device=is_eos.device))
            #è®¡ç®—å½“å‰batchä¸­æ ·æœ¬ç”Ÿæˆé•¿åº¦çš„å¹³å‡å€¼
            avg_len = lengths.float().mean()

            # æå– scalar å€¼ä»¥ä¾¿æ‰“å°
            actor_loss_val = policy_loss.item()
            critic_loss_val = value_loss.item()
            reward_val = rewards.mean().item()
            kl_val = kl.item()
            kl_ref_val = kl_ref.item()
            avg_len_val = avg_len.item()
            actor_lr = actor_optimizer.param_groups[0]['lr']
            critic_lr = critic_optimizer.param_groups[0]['lr']
            # wandbç”¨äºç”»å›¾
            if wandb is not None:
                wandb.log({
                    "actor_loss": actor_loss_val,
                    "critic_loss": critic_loss_val,
                    "reward": reward_val,
                    "kl": kl_val,
                    "kl_ref": kl_ref_val,
                    "avg_response_len": avg_len_val,
                    "actor_lr": actor_lr,
                })
            # æ—¥å¿—ä¿¡æ¯
            Logger(f"Epoch: {epoch+1}, Step: {step}/{iters}, "
                   f"Actor Loss: {actor_loss_val:.6f}, Critic Loss: {critic_loss_val:.6f}, "
                   f"Reward: {reward_val:.6f}, KL: {kl_val:.6f}, KL_ref: {kl_ref_val:.6f}, "
                   f"Avg Response Len: {avg_len_val:.2f}, Actor LR: {actor_lr:.2e}, Critic LR: {critic_lr:.2e}")

        # Actoræ¨¡å‹çš„å»¶è¿Ÿæ›´æ–°, æ¯update_old_actor_freqæ‰æ›´æ–°ä¸€æ¬¡
        if (step + 1) % args.update_old_actor_freq == 0:
            state_dict = actor_model.module.state_dict() if isinstance(actor_model, DistributedDataParallel) else actor_model.state_dict()
            old_actor_model.load_state_dict({k: v.detach().cpu() for k, v in state_dict.items()})
            old_actor_model.to(args.device)

        # ä¿å­˜æ¨¡å‹æƒé‡
        if (step % args.save_interval == 0 or step == iters - 1) and is_main_process():
            # åˆ‡æ¢ä¸ºevalæ¨¡å‹, ä¼šå…³é—­ Dropout å±‚ï¼Œå¹¶å›ºå®š BatchNorm çš„ç»Ÿè®¡é‡ï¼Œç¡®ä¿ä¿å­˜çš„å‚æ•°æ˜¯ç¨³å®šçš„ã€‚
            actor_model.eval()
            moe_suffix = '_moe' if lm_config.use_moe else ''
            ckp = f'{args.save_dir}/{args.save_weight}_{lm_config.hidden_size}{moe_suffix}.pth'
            actor_state = actor_model.module.state_dict() if isinstance(actor_model, DistributedDataParallel) else actor_model.state_dict()
            torch.save({k: v.half() for k, v in actor_state.items()}, ckp)
            
            # ä¿å­˜æ¢å¤è®­ç»ƒæ‰€éœ€è¦çš„ä¸€åˆ‡
            lm_checkpoint(lm_config, weight=args.save_weight, model=actor_model, optimizer=actor_optimizer, 
                         epoch=epoch, step=step, wandb=wandb, save_dir='../checkpoints',
                         scheduler=actor_scheduler, critic_model=critic_model, 
                         critic_optimizer=critic_optimizer, critic_scheduler=critic_scheduler)
            # é‡æ–°å›åˆ°è®­ç»ƒæ¨¡å¼
            actor_model.train()

```

ä»¥ä¸‹æ˜¯å°† `train_ppo.py` ä¸»ç¨‹åºå…¥å£éƒ¨åˆ†ï¼ˆ`if __name__ == "__main__":`ï¼‰æŒ‰ç…§ä»£ç ä¸­çš„æ³¨é‡Šå—è¿›è¡Œçš„ Markdown åˆ†å—è§£æï¼š

### å‚æ•°è§£æä¸å…¨å±€é…ç½®

è¿™ä¸€éƒ¨åˆ†å®šä¹‰äº†è„šæœ¬è¿è¡Œæ‰€éœ€çš„æ‰€æœ‰è¶…å‚æ•°ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ã€æ¨¡å‹å‚æ•°ã€è·¯å¾„é…ç½®ä»¥åŠ PPO ç‰¹æœ‰çš„ç³»æ•°ï¼ˆå¦‚ `clip_epsilon`, `kl_coef`ï¼‰ã€‚

```python
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MiniMind PPO (Proximal Policy Optimization)")
    parser.add_argument("--save_dir", type=str, default="../out", help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument('--save_weight', default='ppo_actor', type=str, help="ä¿å­˜æƒé‡çš„å‰ç¼€å")
    parser.add_argument("--epochs", type=int, default=1, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=2, help="batch size")
    parser.add_argument("--learning_rate", type=float, default=8e-8, help="Actorå­¦ä¹ ç‡")
    parser.add_argument("--critic_learning_rate", type=float, default=8e-8, help="Criticå­¦ä¹ ç‡")
    parser.add_argument("--device", type=str, default="cuda:0" if torch.cuda.is_available() else "cpu", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--dtype", type=str, default="bfloat16", help="æ··åˆç²¾åº¦ç±»å‹")
    parser.add_argument("--num_workers", type=int, default=1, help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")
    parser.add_argument("--accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    parser.add_argument("--log_interval", type=int, default=1, help="æ—¥å¿—æ‰“å°é—´éš”")
    parser.add_argument("--save_interval", type=int, default=10, help="æ¨¡å‹ä¿å­˜é—´éš”")
    parser.add_argument('--hidden_size', default=512, type=int, help="éšè—å±‚ç»´åº¦")
    parser.add_argument('--num_hidden_layers', default=8, type=int, help="éšè—å±‚æ•°é‡")
    parser.add_argument('--use_moe', default=0, type=int, choices=[0, 1], help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    parser.add_argument('--max_seq_len', default=66, type=int, help="Promptæœ€å¤§é•¿åº¦")
    parser.add_argument("--max_gen_len", type=int, default=1536, help="ç”Ÿæˆçš„æœ€å¤§é•¿åº¦")
    parser.add_argument("--data_path", type=str, default="../dataset/rlaif-mini.jsonl", help="RLAIFæ•°æ®è·¯å¾„")
    parser.add_argument("--clip_epsilon", type=float, default=0.1, help="PPOè£å‰ªå‚æ•°")
    parser.add_argument("--vf_coef", type=float, default=0.5, help="Value functionç³»æ•°")
    parser.add_argument("--kl_coef", type=float, default=0.02, help="KLæ•£åº¦æƒ©ç½šç³»æ•°")
    parser.add_argument("--reasoning", type=int, default=1, choices=[0, 1], help='æ¨ç†æ¨¡å‹ç±»å‹ï¼ˆ0=æ™®é€šæ¨¡å‹ï¼Œ1=æ¨ç†æ¨¡å‹ï¼‰')
    parser.add_argument("--update_old_actor_freq", type=int, default=4, help="æ›´æ–°old_actor_modelçš„é¢‘ç‡")
    parser.add_argument("--reward_model_path", type=str, default="../../internlm2-1_8b-reward", help="Rewardæ¨¡å‹è·¯å¾„")
    parser.add_argument('--from_resume', default=0, type=int, choices=[0, 1], help="æ˜¯å¦è‡ªåŠ¨æ£€æµ‹&ç»­è®­ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    parser.add_argument("--use_wandb", action="store_true", help="æ˜¯å¦ä½¿ç”¨wandb")
    parser.add_argument("--wandb_project", type=str, default="MiniMind-PPO", help="wandbé¡¹ç›®å")
    args = parser.parse_args()

```
## ä¸»ç¨‹åºå…¥å£
### 1. åˆå§‹åŒ–ç¯å¢ƒå’Œéšæœºç§å­

åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼ˆå¦‚æœéœ€è¦ï¼‰ï¼Œå¹¶è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒçš„å¯å¤ç°æ€§ã€‚

```python
    local_rank = init_distributed_mode()
    if dist.is_initialized(): args.device = f"cuda:{local_rank}"
    setup_seed(42 + (dist.get_rank() if dist.is_initialized() else 0))

```

### 2. é…ç½®ç›®å½•ã€æ¨¡å‹å‚æ•°ã€æ£€æŸ¥ckp

åˆ›å»ºä¿å­˜ç›®å½•ï¼Œå®ä¾‹åŒ–æ¨¡å‹é…ç½®å¯¹è±¡ `MiniMindConfig`ã€‚å¦‚æœå¼€å¯äº† `from_resume`ï¼Œåˆ™ä¼šå°è¯•å¯»æ‰¾ä¹‹å‰ä¿å­˜çš„ checkpoint ä¿¡æ¯ã€‚

```python
    os.makedirs(args.save_dir, exist_ok=True)
    lm_config = MiniMindConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers, use_moe=bool(args.use_moe))
    ckp_data = lm_checkpoint(lm_config, weight=args.save_weight, save_dir='../checkpoints') if args.from_resume==1 else None

```

### 3. è®¾ç½®æ··åˆç²¾åº¦

æ ¹æ®è®¾å¤‡ç±»å‹å’Œå‚æ•°è®¾ç½®è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡ï¼ˆAMPï¼‰ï¼Œé€šå¸¸ä½¿ç”¨ `bfloat16` æˆ– `float16` ä»¥èŠ‚çœæ˜¾å­˜å¹¶åŠ é€Ÿè®­ç»ƒã€‚

```python
    # ========== 3. è®¾ç½®æ··åˆç²¾åº¦ ==========
    device_type = "cuda" if "cuda" in args.device else "cpu"
    dtype = torch.bfloat16 if args.dtype == "bfloat16" else torch.float16
    autocast_ctx = nullcontext() if device_type == "cpu" else torch.cuda.amp.autocast(dtype=dtype)

```

### 4. é…ç½®wandb

åˆå§‹åŒ– Weights & Biases (WandB) æˆ– SwanLab ç”¨äºå®éªŒç›‘æ§ã€‚å¦‚æœä»æ–­ç‚¹æ¢å¤ï¼Œä¼šå°è¯•æ¢å¤å¯¹åº”çš„ run IDã€‚

```python
    wandb = None
    if args.use_wandb and is_main_process():
        import swanlab as wandb
        wandb_id = ckp_data.get('wandb_id') if ckp_data else None
        resume = 'must' if wandb_id else None
        wandb_run_name = f"MiniMind-PPO-Epoch-{args.epochs}-BS-{args.batch_size}-LR-{args.learning_rate}"
        wandb.init(project=args.wandb_project, name=wandb_run_name, id=wandb_id, resume=resume)

```

### 5. åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®

è¿™æ˜¯æœ€å…³é”®çš„åˆå§‹åŒ–æ­¥éª¤ï¼Œæ„å»ºäº† PPO æ‰€éœ€çš„å››ä¸ªæ¨¡å‹ï¼š

1. **Actor Model**: å½“å‰è®­ç»ƒçš„ç­–ç•¥ç½‘ç»œã€‚
2. **Old Actor**: ç”¨äºè®¡ç®—æ¯”ç‡ï¼ˆRatioï¼‰çš„æ—§ç­–ç•¥ç½‘ç»œï¼ˆå†»ç»“å‚æ•°ï¼‰ã€‚
3. **Reference Model**: ç”¨äºè®¡ç®— KL æ•£åº¦çš„å‚è€ƒç½‘ç»œï¼ˆå†»ç»“å‚æ•°ï¼‰ã€‚
4. **Critic Model**: ä»·å€¼ç½‘ç»œï¼Œç”¨äºä¼°è®¡çŠ¶æ€ä»·å€¼ã€‚
æ­¤å¤–ï¼Œè¿˜åŠ è½½äº† **Reward Model**ã€æ•°æ®é›†å’Œä¼˜åŒ–å™¨ã€‚

```python
    # åŠ è½½æ¨¡å‹æƒé‡
    base_weight = "reason" if args.reasoning == 1 else "full_sft"
    # Actoræ¨¡å‹
    actor_model, tokenizer = init_model(lm_config, base_weight, device=args.device)
    # ç”Ÿæˆå¼ä»»åŠ¡ä¸­ï¼Œå› ä¸ºç”Ÿæˆçš„Tokenéƒ½æ˜¯å‘å³è¿½åŠ çš„ï¼Œæ‰€ä»¥è¦å·¦å¯¹é½
    tokenizer.padding_side = 'left'  
    # Old Actoræ¨¡å‹
    old_actor_model, _ = init_model(lm_config, base_weight, device=args.device)
    # å†»ç»“å‚æ•°ï¼Œä¸å‚ä¸è®­ç»ƒ
    old_actor_model = old_actor_model.eval().requires_grad_(False)
    # Referenceæ¨¡å‹
    ref_model, _ = init_model(lm_config, base_weight, device=args.device)
    ref_model = ref_model.eval().requires_grad_(False)
    # Criticæ¨¡å‹
    moe_suffix = '_moe' if lm_config.use_moe else ''
    ckp = f'{args.save_dir}/{base_weight}_{lm_config.hidden_size}{moe_suffix}.pth'
    state_dict = torch.load(ckp, map_location=args.device)
    critic_model = CriticModel(lm_config)
    critic_model.load_state_dict(state_dict, strict=False)
    critic_model = critic_model.to(args.device)
    # Rewardæ¨¡å‹
    reward_model = AutoModel.from_pretrained(
        args.reward_model_path, torch_dtype=torch.float16, trust_remote_code=True
    )
    reward_model = reward_model.to(args.device).eval().requires_grad_(False)
    reward_tokenizer = AutoTokenizer.from_pretrained(args.reward_model_path, trust_remote_code=True)
    # æ•°æ®å’Œä¼˜åŒ–å™¨
    # åŠ è½½æç¤ºè¯æ•°æ®é›†
    train_ds = RLAIFDataset(args.data_path, tokenizer, max_length=(args.max_seq_len + args.max_gen_len))
    train_sampler = DistributedSampler(train_ds) if dist.is_initialized() else None
    # ä¼˜åŒ–å™¨é…ç½®
    actor_optimizer = optim.AdamW(actor_model.parameters(), lr=args.learning_rate)
    critic_optimizer = optim.AdamW(critic_model.parameters(), lr=args.critic_learning_rate)
    # å­¦ä¹ ç‡è°ƒåº¦ä¸åˆå§‹åŒ–
    loader_for_count = DataLoader(train_ds, batch_size=args.batch_size, sampler=train_sampler)
    iters = len(loader_for_count)
    total_optimizer_steps = (iters // args.accumulation_steps) * args.epochs
    actor_scheduler = CosineAnnealingLR(actor_optimizer, T_max=total_optimizer_steps, eta_min=args.learning_rate / 10)
    critic_scheduler = CosineAnnealingLR(critic_optimizer, T_max=total_optimizer_steps, eta_min=args.critic_learning_rate / 10)

```

### 6. ä»ckpæ¢å¤çŠ¶æ€

å¦‚æœæ£€æµ‹åˆ° checkpoint æ•°æ®ï¼Œå°†æ‰€æœ‰æ¨¡å‹ï¼ˆActor, Criticï¼‰ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çš„çŠ¶æ€æ¢å¤åˆ°ä¹‹å‰ä¿å­˜çš„ç‚¹ã€‚

```python
    # ä»checkpointæ¢å¤çŠ¶æ€
    start_epoch, start_step = 0, 0
    if ckp_data:
        actor_model.load_state_dict(ckp_data['model'])
        critic_model.load_state_dict(ckp_data['critic_model'])
        actor_optimizer.load_state_dict(ckp_data['optimizer'])
        critic_optimizer.load_state_dict(ckp_data['critic_optimizer'])
        actor_scheduler.load_state_dict(ckp_data['scheduler'])
        critic_scheduler.load_state_dict(ckp_data['critic_scheduler'])
        start_epoch = ckp_data['epoch']
        start_step = ckp_data.get('step', 0)

```

### 7. DDP åˆ†å¸ƒå¼å°è£…

å¦‚æœæ˜¯åˆ†å¸ƒå¼è®­ç»ƒï¼Œä½¿ç”¨ `DistributedDataParallel` (DDP) å°è£… Actor å’Œ Critic æ¨¡å‹ï¼ŒåŒæ—¶å¿½ç•¥ç‰¹å®šçš„ MoE å‚æ•°ï¼ˆå¦‚ `freqs_cos`ï¼‰ä»¥é¿å…å¹¿æ’­é”™è¯¯ã€‚

```python
    if dist.is_initialized():
        actor_model._ddp_params_and_buffers_to_ignore = {"freqs_cos", "freqs_sin"}
        critic_model._ddp_params_and_buffers_to_ignore = {"freqs_cos", "freqs_sin"}
        actor_model = DistributedDataParallel(actor_model, device_ids=[local_rank])
        critic_model = DistributedDataParallel(critic_model, device_ids=[local_rank])
        old_actor_model.to(args.device)

```

### 8. å¼€å§‹è®­ç»ƒ

è¿›å…¥ä¸»è®­ç»ƒå¾ªç¯ã€‚è¿™é‡Œå¤„ç†äº†æ–­ç‚¹ç»­è®­æ—¶çš„æ•°æ®åŠ è½½å™¨è·³è¿‡é€»è¾‘ï¼ˆ`SkipBatchSampler`ï¼‰ï¼Œå¹¶è°ƒç”¨æ ¸å¿ƒå‡½æ•° `ppo_train_epoch` å¼€å§‹ PPO çš„ Epoch è®­ç»ƒã€‚

```python
    # å¼€å§‹è®­ç»ƒ
    for epoch in range(start_epoch, args.epochs):
        train_sampler and train_sampler.set_epoch(epoch)
        if epoch == start_epoch and start_step > 0:  # ç¬¬ä¸€ä¸ªepochä¸”å­˜åœ¨æ£€æŸ¥ç‚¹
            batch_sampler = SkipBatchSampler(train_sampler or range(len(train_ds)), args.batch_size, start_step + 1)
            loader = DataLoader(train_ds, batch_sampler=batch_sampler, num_workers=args.num_workers, pin_memory=True)
            Logger(f'Epoch [{epoch + 1}/{args.epochs}]: è·³è¿‡å‰{start_step}ä¸ªstepï¼Œä»step {start_step + 1}å¼€å§‹')
            ppo_train_epoch(epoch, loader, len(loader) + start_step + 1, old_actor_model, ref_model, 
                           actor_scheduler, critic_scheduler, reward_model, reward_tokenizer, start_step, wandb)
        else:  # é»˜è®¤ä»å¤´å¼€å§‹
            loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=(train_sampler is None), 
                              sampler=train_sampler, num_workers=args.num_workers, pin_memory=True)
            ppo_train_epoch(epoch, loader, len(loader), old_actor_model, ref_model, 
                           actor_scheduler, critic_scheduler, reward_model, reward_tokenizer, 0, wandb)

```


> "åƒé‡Œä¹‹è¡Œï¼Œå§‹äºè¶³ä¸‹ã€‚"